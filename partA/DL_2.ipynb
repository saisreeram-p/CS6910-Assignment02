{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jsDALniQnzj"
      },
      "source": [
        "# Installing Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0yIjToZfKkz"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU\n",
        "!pip install pytorch_lightning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am7fPcNHQvKJ"
      },
      "source": [
        "# Importing Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq2lHM9cPsy1"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import torch \n",
        "import pytorch_lightning as pl\n",
        "import torch.nn as nn\n",
        "from torch.nn  import functional\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIL_C-NXQ0Mz"
      },
      "source": [
        "# Downloading Unzip data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybVd9PVufOhI"
      },
      "outputs": [],
      "source": [
        "!curl -SL https://storage.googleapis.com/wandb_datasets/nature_12K.zip > nature_12K.zip\n",
        "!unzip nature_12K.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxdJSVilWcQC"
      },
      "source": [
        "# Connecting Wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1MtE97XVbTJV",
        "outputId": "736b7f92-79ec-449c-c89f-713096a9c963"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m076\u001b[0m (\u001b[33msaisreeram\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_100433-y3lcu76r</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/saisreeram/Assignment-02/runs/y3lcu76r' target=\"_blank\">grateful-silence-1514</a></strong> to <a href='https://wandb.ai/saisreeram/Assignment-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/saisreeram/Assignment-02' target=\"_blank\">https://wandb.ai/saisreeram/Assignment-02</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/saisreeram/Assignment-02/runs/y3lcu76r' target=\"_blank\">https://wandb.ai/saisreeram/Assignment-02/runs/y3lcu76r</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/saisreeram/Assignment-02/runs/y3lcu76r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f986e2e9c70>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "wandb.login(key=\"8d6c17aa48af2229c26cbc16513ef266358c0b96\")\n",
        "wandb.init(project=\"Assignment-02\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hhOmSKoWwCF"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XyF0IZYf3Cn"
      },
      "outputs": [],
      "source": [
        "baseDir = \"inaturalist_12K\"\n",
        "trainDir = baseDir+\"/train/\"\n",
        "testDir = baseDir+\"/val/\"\n",
        "outputclasses=[\"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\", \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"]\n",
        "input_size = 256\n",
        "batch_size = 16 \n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((input_size,input_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=baseDir+'/train',transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=baseDir+'/val',transform=transform)\n",
        " \n",
        "trainSize = int(0.8 * len(train_dataset))\n",
        "valSize = len(train_dataset) - trainSize\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [trainSize, valSize])\n",
        "\n",
        "train_dataset = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           )\n",
        "\n",
        "val_dataset = torch.utils.data.DataLoader(val_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=False,\n",
        "                                           )\n",
        "\n",
        "test_dataset = torch.utils.data.DataLoader(test_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=False,\n",
        "                                           )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LKEhkZmXv7n"
      },
      "source": [
        "# CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSic2xrLTnyU"
      },
      "outputs": [],
      "source": [
        "class CNN(pl.LightningModule):\n",
        "  def __init__(self,filters,activation,BatchNorm,dropout,learning_rate,input_size,kernel_size,pool_kernel_size,pool_stride):\n",
        "    dense_size = input_size\n",
        "    for i in filters:\n",
        "      dense_size = (dense_size-kernel_size+1-pool_kernel_size)//pool_stride +1\n",
        "    self.dense_size = dense_size\n",
        "    super(CNN,self).__init__()\n",
        "    self.train_step_acc = []\n",
        "    self.train_step_loss = []\n",
        "    self.val_step_acc = []\n",
        "    self.val_step_loss = []\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    layers = []\n",
        "    layers.append(nn.Conv2d(3,filters[0],kernel_size = kernel_size,stride = 1,padding = 0))\n",
        "    layers.append(nn.MaxPool2d(kernel_size = pool_kernel_size,stride = pool_stride))\n",
        "    layers.append(activation)\n",
        "    for i in range(0,3):\n",
        "      layers.append(nn.Conv2d(filters[i],filters[i+1],kernel_size = kernel_size,stride = 1,padding = 0))\n",
        "      layers.append(nn.MaxPool2d(kernel_size = pool_kernel_size,stride = pool_stride))\n",
        "      layers.append( activation)\n",
        "\n",
        "    layers.append(nn.Conv2d(filters[3],filters[4],kernel_size = kernel_size,stride = 1,padding = 0))\n",
        "    layers.append(nn.MaxPool2d(kernel_size = pool_kernel_size,stride = pool_stride))\n",
        "    layers.append( activation)\n",
        "    layers.append(nn.Flatten())\n",
        "    \n",
        "    if(BatchNorm == True):\n",
        "      layers.append(nn.BatchNorm1d(filters[4]*self.dense_size*self.dense_size))\n",
        "    layers.append(nn.Dropout(p=dropout))\n",
        "\n",
        "    layers.append(nn.Linear(filters[4]*self.dense_size*self.dense_size,256 ))\n",
        "    layers.append( activation)\n",
        "    if(BatchNorm == True):\n",
        "      layers.append(nn.BatchNorm1d(256))\n",
        "    layers.append(nn.Dropout(p=dropout))\n",
        "    layers.append(nn.Linear(256,10 ))\n",
        "\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "        \n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters(),lr= self.learning_rate)\n",
        "\n",
        "  def training_step(self,batch):\n",
        "    trainX,trainY = batch\n",
        "    output = self(trainX)\n",
        "    loss = self.loss(output,trainY)\n",
        "    acc = (output.argmax(dim = 1) == trainY).float().mean()\n",
        "    self.train_step_acc.append(acc)\n",
        "    self.train_step_loss.append(loss)\n",
        "\n",
        "    self.log('train_loss1', loss,on_epoch = True,on_step = False,prog_bar=True,metric_attribute=\"train_loss\")\n",
        "    self.log('train_acc1', acc,on_epoch = True,on_step = False,prog_bar=True,metric_attribute=\"train_acc\")\n",
        "    return loss\n",
        "\n",
        "  def on_train_epoch_end(self):\n",
        "    \n",
        "    train_acc =  torch.stack(self.train_step_acc).mean()\n",
        "    train_loss =  torch.stack(self.train_step_loss).mean()\n",
        "    val_acc =  torch.stack(self.val_step_acc).mean()\n",
        "    val_loss =  torch.stack(self.val_step_loss).mean()\n",
        "\n",
        "    # wandb.log({\"train_loss\":train_loss.item(),\"train_acc\":train_acc.item(),\"val_loss\":val_loss.item(),\"val_acc\":val_acc.item()})\n",
        "    self.train_step_acc.clear() \n",
        "    self.train_step_loss.clear() \n",
        "    self.val_step_acc.clear() \n",
        "    self.val_step_loss.clear() \n",
        "\n",
        "\n",
        "  def validation_step(self, batch,batch_idx):\n",
        "    trainX,trainY = batch\n",
        "    output = self(trainX)\n",
        "    loss = self.loss(output,trainY)\n",
        "    acc = (output.argmax(dim = 1) == trainY).float().mean()\n",
        "    self.val_step_acc.append(acc)\n",
        "    self.val_step_loss.append(loss)\n",
        "    self.log('val_loss1', loss,on_epoch = True,on_step = False,prog_bar=True,sync_dist=True)\n",
        "    self.log('val_acc1', acc,on_epoch = True,on_step = False,prog_bar=True,sync_dist=True)\n",
        "    return loss\n",
        "\n",
        "  def test_step(self, batch,batch_idx):\n",
        "    trainX,trainY = batch\n",
        "    output = self(trainX)\n",
        "    loss = self.loss(output,trainY)\n",
        "    acc = (output.argmax(dim = 1) == trainY).float().mean()\n",
        "    self.log('test_loss', loss,on_epoch = True,on_step = False,prog_bar=True)\n",
        "    self.log('test_acc', acc,on_epoch = True,on_step = False,prog_bar=True)\n",
        "    return loss\n",
        "\n",
        "  def predict_step(self, batch,batch_idx,dataloader_idx=0):\n",
        "    trainX = batch\n",
        "    output = self(trainX)\n",
        "    return output.argmax(dim = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGn-q7HuYuho"
      },
      "source": [
        "# Sweep Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsX9MeVTEli8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "config= {\n",
        "    'method': 'bayes',\n",
        "    'name': 'sweep',\n",
        "    'metric': {\n",
        "        'goal': 'maximize', \n",
        "        'name': 'val_acc'\n",
        "      },\n",
        "    \"parameters\":\n",
        "    {\n",
        "    \"filters\":{\n",
        "      \"values\" :[[64,64,64,64,64],[32,32,32,32,32],[4,8,16,32,64],[64,32,16,8,4]]\n",
        "    },\n",
        "    \"data_augmentation\" :{\n",
        "        \"values\" : [True,False]\n",
        "    },\n",
        "    \"batch_normalisation\" :{\n",
        "        \"values\" : [True,False]\n",
        "    },\n",
        "    \"dropout\" :{\n",
        "        \"values\" : [0.2,0.3,0.4]\n",
        "    },\n",
        "    \"activation\" :{\n",
        "          \"values\" : [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n",
        "    },\n",
        "      \"epochs\" :{\n",
        "          \"values\" : [5,  10]\n",
        "    },\n",
        "      \"kernel_size\" :{\n",
        "          \"values\" : [3,4,5]\n",
        "    },\n",
        "      \"pool_kernel_size\" :{\n",
        "          \"values\" : [2,  3]\n",
        "    },\n",
        "\n",
        "      \"learning_rate\" :{\n",
        "          \"values\" : [1e-3,1e-4]\n",
        "    }\n",
        "\n",
        "    }\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PbMUSStY4rm"
      },
      "source": [
        "# Sweep Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLgcFoCIFOh1"
      },
      "outputs": [],
      "source": [
        "\n",
        "activation_map = {\"ReLU\":nn.ReLU(), \"GELU\":nn.GELU(), \"SiLU\":nn.SiLU(), \"Mish\":nn.Mish()}\n",
        "def sweeprun():\n",
        "\n",
        "  wandb.init()\n",
        "  filters = wandb.config.filters\n",
        "  data_augmentation = wandb.config.data_augmentation\n",
        "  batch_normalisation = wandb.config.batch_normalisation\n",
        "  dropout = wandb.config.dropout\n",
        "  activation = wandb.config.activation\n",
        "  epochs = wandb.config.epochs\n",
        "  learning_rate = wandb.config.learning_rate\n",
        "  kernel_size = wandb.config.kernel_size\n",
        "  pool_kernel_size = wandb.config.pool_kernel_size\n",
        "  pool_stride = 2\n",
        "\n",
        "  transform_aug = transforms.Compose([\n",
        "            transforms.Resize((input_size,input_size)),\n",
        "            transforms.AutoAugment(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "  transform_basic = transforms.Compose([\n",
        "              transforms.Resize((input_size,input_size)),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "          ])\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(root=baseDir+'/train',\n",
        "                                            transform=transform_basic)\n",
        "\n",
        "  trainSize = int(0.8 * len(train_dataset))\n",
        "  valSize = len(train_dataset) - trainSize\n",
        "\n",
        "  train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [trainSize, valSize])\n",
        "  if data_augmentation == True :\n",
        "    train_dataset.transform = transform_aug\n",
        "  train_dataset = torch.utils.data.DataLoader(train_dataset,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=True,\n",
        "                                            )\n",
        "\n",
        "  val_dataset = torch.utils.data.DataLoader(val_dataset,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False,\n",
        "                                            )\n",
        "\n",
        "  run_name = \"lr_{}_ac_{}_dp_{}_bn_{}_fi_{}_da_{}_ks_{}_pks_{}\".format(learning_rate,activation, dropout, batch_normalisation,filters, data_augmentation,kernel_size,pool_kernel_size)\n",
        "\n",
        "  activation = activation_map[activation]\n",
        "  clf = CNN(filters,activation,batch_normalisation,dropout,learning_rate,input_size,kernel_size,pool_kernel_size,pool_stride) \n",
        "  trainer = pl.Trainer(max_epochs=epochs) \n",
        "  trainer.fit(clf,train_dataset,val_dataset)\n",
        "  wandb.run.name = run_name\n",
        "  wandb.finish()\n",
        "\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(config,project=\"Assignment-02-sweep\", entity = \"saisreeram\")\n",
        "wandb.agent(sweep_id, sweeprun)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best model on the test data"
      ],
      "metadata": {
        "id": "_2yFipRE20gJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_XO-P96L5s2"
      },
      "outputs": [],
      "source": [
        "data_augmentation = True\n",
        "transform_aug = transforms.Compose([\n",
        "            transforms.Resize((input_size,input_size)),\n",
        "            transforms.AutoAugment(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "if data_augmentation == True :\n",
        "    train_dataset.transform = transform_aug\n",
        "\n",
        "clf = CNN([64,64,64,64,64],nn.GELU(),True,0.3,1e-4,input_size,3,3,2) \n",
        "trainer = pl.Trainer(max_epochs=5) #,accelerator='gpu'\n",
        "trainer.fit(clf,train_dataset,val_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(clf, test_dataset)\n"
      ],
      "metadata": {
        "id": "X-wVAWpW29Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample images from the test data and predictions"
      ],
      "metadata": {
        "id": "dgD89RLB1qYO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_DgIvBsgPaP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "wanglog = []\n",
        "transform = transforms.Compose([\n",
        "              transforms.Resize((input_size,input_size)),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "          ])\n",
        "\n",
        "plt.figure(figsize=[15, 40])\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "clf.eval()\n",
        "for i in range(0,10):\n",
        "  for j in range(0,3):\n",
        "    plt.subplot(10,3,i*3+j+1)\n",
        "    folder_path = \"inaturalist_12K/train/\"+outputclasses[i]\n",
        "    files = os.listdir(folder_path)\n",
        "    random_file = random.choice(files)\n",
        "    image = Image.open(folder_path+'/'+random_file)\n",
        "    image_tensor = transform(image).to(device)\n",
        "    image_tensor = image_tensor.reshape(1,3,256,256)\n",
        "    plt.xlabel('True Label : '+outputclasses[i])\n",
        "    plt.ylabel('Predicted Label : '+outputclasses[clf.forward(image_tensor).argmax(dim = 1).item()])\n",
        "    plt.tick_params(left = False, right = False , labelleft = False ,\n",
        "                labelbottom = False, bottom = False)\n",
        "    plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)\n",
        "    plt.imshow(image)\n",
        "\n",
        "wandb.log({\"Question 4\": wandb.Image(plt)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOU3x126p0RU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}